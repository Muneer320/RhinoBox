# Demo Script

This document provides a comprehensive demo script for showcasing RhinoBox capabilities in a hackathon presentation.

## Demo Overview

**Duration**: 5-10 minutes  
**Scenarios**: 7 key demonstrations  
**Objective**: Showcase intelligent routing, performance, and ease of use

---

## Setup (Before Demo)

```bash
# 1. Start all services
docker-compose up -d

# 2. Wait for services to be healthy (30 seconds)
sleep 30

# 3. Verify services
curl http://localhost:8090/healthz
# Expected: {"status":"ok"}

# 4. Prepare test files
mkdir -p demo_files
cd demo_files

# Download sample files or use your own:
# - photo.jpg (image)
# - video.mp4 (video)
# - document.pdf (PDF)
# - data.json (JSON file)
```

---

## Demo Script

### ðŸŽ¬ Scenario 1: Upload Image File

**Objective**: Show automatic MIME detection and type-based organization

```bash
# Upload an image
curl -X POST http://localhost:8090/ingest \
  -F "files=@photo.jpg" \
  -F "category=vacation_photos" \
  -F "comment=Beach sunset, Maldives 2025"
```

**Expected Response**:
```json
{
  "status": "completed",
  "results": {
    "media": [
      {
        "original_name": "photo.jpg",
        "stored_path": "storage/media/images/jpg/vacation_photos/abc123def...photo.jpg",
        "hash": "abc123def456...",
        "size": 2457600,
        "mime_type": "image/jpeg",
        "category": "images/jpg/vacation_photos",
        "uploaded_at": "2025-11-15T10:30:00Z",
        "is_duplicate": false
      }
    ]
  }
}
```

**Highlight**:
- âœ… MIME type auto-detected: `image/jpeg`
- âœ… Organized: `storage/media/images/jpg/vacation_photos/`
- âœ… SHA-256 hash for deduplication
- âœ… Response in <50ms

**Show**:
```bash
# Verify file was stored
ls -lh storage/media/images/jpg/vacation_photos/
# Shows: abc123def...photo.jpg
```

---

### ðŸŽ¬ Scenario 2: Upload Video File

**Objective**: Show multi-type support

```bash
# Upload a video
curl -X POST http://localhost:8090/ingest \
  -F "files=@video.mp4" \
  -F "comment=Tutorial screencast"
```

**Expected Response**:
```json
{
  "status": "completed",
  "results": {
    "media": [
      {
        "original_name": "video.mp4",
        "stored_path": "storage/media/videos/mp4/video.mp4",
        "mime_type": "video/mp4",
        "category": "videos/mp4",
        "size": 157286400,
        "is_duplicate": false
      }
    ]
  }
}
```

**Highlight**:
- âœ… Automatic video classification
- âœ… Organized: `storage/media/videos/mp4/`
- âœ… Large files supported (100MB+)

---

### ðŸŽ¬ Scenario 3: JSON â†’ SQL (Relational Data)

**Objective**: Demonstrate intelligent SQL routing for structured data

```bash
# Upload structured JSON with relationships
curl -X POST http://localhost:8090/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "namespace": "orders",
    "comment": "E-commerce orders with customer references",
    "documents": [
      {
        "order_id": 1001,
        "customer_id": 5001,
        "product": "Laptop",
        "quantity": 1,
        "price": 999.99,
        "order_date": "2025-11-15T10:00:00Z",
        "status": "pending"
      },
      {
        "order_id": 1002,
        "customer_id": 5002,
        "product": "Mouse",
        "quantity": 2,
        "price": 29.99,
        "order_date": "2025-11-15T10:05:00Z",
        "status": "shipped"
      },
      {
        "order_id": 1003,
        "customer_id": 5001,
        "product": "Keyboard",
        "quantity": 1,
        "price": 79.99,
        "order_date": "2025-11-15T10:10:00Z",
        "status": "delivered"
      }
    ]
  }'
```

**Expected Response**:
```json
{
  "status": "completed",
  "results": {
    "json": {
      "namespace": "orders",
      "engine": "sql",
      "table": "orders",
      "rows_inserted": 3,
      "database": "postgresql",
      "confidence": 0.95,
      "decision_reasons": [
        "Stable schema detected (100% consistency)",
        "Foreign key pattern: customer_id",
        "Shallow nesting (depth 1)",
        "Numeric ID fields present"
      ]
    }
  }
}
```

**Highlight**:
- âœ… Automatic SQL detection
- âœ… Reason: Stable schema + `customer_id` foreign key pattern
- âœ… PostgreSQL table created automatically
- âœ… 100K+ inserts/sec with COPY protocol

**Verify in Database**:
```bash
# Query PostgreSQL
docker exec -it rhinobox-postgres psql -U rhinobox -d rhinobox -c "SELECT * FROM orders;"

# Expected output:
#  order_id | customer_id | product  | quantity |  price  | order_date          | status
# ----------+-------------+----------+----------+---------+---------------------+-----------
#  1001     | 5001        | Laptop   | 1        | 999.99  | 2025-11-15 10:00:00 | pending
#  1002     | 5002        | Mouse    | 2        | 29.99   | 2025-11-15 10:05:00 | shipped
#  1003     | 5001        | Keyboard | 1        | 79.99   | 2025-11-15 10:10:00 | delivered
```

---

### ðŸŽ¬ Scenario 4: JSON â†’ NoSQL (Flexible Documents)

**Objective**: Demonstrate intelligent NoSQL routing for varied schemas

```bash
# Upload flexible JSON with inconsistent structure
curl -X POST http://localhost:8090/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "namespace": "activity_logs",
    "comment": "Flexible event logs with varying structure",
    "documents": [
      {
        "user": {
          "id": "u1001",
          "name": "Alice",
          "email": "alice@example.com"
        },
        "events": [
          {"type": "click", "target": "button#submit", "timestamp": "2025-11-15T10:00:00Z"},
          {"type": "scroll", "position": 1250, "timestamp": "2025-11-15T10:00:05Z"}
        ],
        "session_id": "sess_abc123",
        "metadata": {
          "ip": "192.168.1.10",
          "user_agent": "Chrome/119.0"
        }
      },
      {
        "user": {"id": "u1002"},
        "events": [
          {"type": "page_view", "url": "/products", "timestamp": "2025-11-15T10:01:00Z"}
        ]
      },
      {
        "user_id": "u1003",
        "action": "login",
        "success": true,
        "timestamp": "2025-11-15T10:02:00Z"
      }
    ]
  }'
```

**Expected Response**:
```json
{
  "status": "completed",
  "results": {
    "json": {
      "namespace": "activity_logs",
      "engine": "nosql",
      "collection": "activity_logs",
      "documents_inserted": 3,
      "database": "mongodb",
      "confidence": 0.92,
      "decision_reasons": [
        "Inconsistent schema (67% consistency)",
        "Deep nesting detected (depth 4)",
        "Array-heavy structure",
        "Comment hint: 'flexible'"
      ]
    }
  }
}
```

**Highlight**:
- âœ… Automatic NoSQL detection
- âœ… Reason: Inconsistent schema + deep nesting + "flexible" hint
- âœ… MongoDB collection created automatically
- âœ… 200K+ inserts/sec with BulkWrite

**Verify in Database**:
```bash
# Query MongoDB
docker exec -it rhinobox-mongo mongosh -u rhinobox -p rhinobox_dev --eval "
  db.getSiblingDB('rhinobox').activity_logs.find().pretty()
"

# Expected: 3 documents with varying structures
```

---

### ðŸŽ¬ Scenario 5: Content Deduplication

**Objective**: Show duplicate detection and storage savings

```bash
# Upload same file twice
curl -X POST http://localhost:8090/ingest \
  -F "files=@photo.jpg" \
  -F "comment=First upload"

curl -X POST http://localhost:8090/ingest \
  -F "files=@photo.jpg" \
  -F "comment=Duplicate upload (same content)"
```

**Expected Response (Second Upload)**:
```json
{
  "status": "completed",
  "results": {
    "media": [
      {
        "original_name": "photo.jpg",
        "stored_path": "storage/media/images/jpg/vacation_photos/abc123def...photo.jpg",
        "hash": "abc123def456...",
        "is_duplicate": true,
        "duplicate_of": "storage/media/images/jpg/vacation_photos/abc123def...photo.jpg"
      }
    ]
  }
}
```

**Highlight**:
- âœ… `is_duplicate: true` flag
- âœ… Returns existing file path (no new storage)
- âœ… Detection in <1ms (L1 cache: 231.5ns)
- âœ… 50%+ storage savings in real-world scenarios

**Show Storage**:
```bash
# Only one file stored despite two uploads
ls -lh storage/media/images/jpg/vacation_photos/ | wc -l
# Expected: 1
```

---

### ðŸŽ¬ Scenario 6: Batch Upload (Async)

**Objective**: Show async processing with zero client blocking

```bash
# Upload 5 files asynchronously
curl -X POST http://localhost:8090/ingest/async \
  -F "files=@photo1.jpg" \
  -F "files=@photo2.jpg" \
  -F "files=@photo3.jpg" \
  -F "files=@video.mp4" \
  -F "files=@document.pdf" \
  -F "comment=Batch upload demo"
```

**Expected Response (Immediate)**:
```json
{
  "job_id": "job_abc123def456",
  "status": "pending",
  "submitted_at": "2025-11-15T10:00:00Z",
  "total_items": 5,
  "progress": 0
}
```

**Highlight**:
- âœ… Response in <1ms (client not blocked)
- âœ… Job ID for tracking progress

**Check Progress**:
```bash
# Poll job status (after 2 seconds)
curl http://localhost:8090/jobs/job_abc123def456
```

**Expected Response (Processing)**:
```json
{
  "job_id": "job_abc123def456",
  "status": "processing",
  "submitted_at": "2025-11-15T10:00:00Z",
  "started_at": "2025-11-15T10:00:01Z",
  "total_items": 5,
  "processed_items": 3,
  "progress": 60,
  "results": {
    "media": [
      {"original_name": "photo1.jpg", "status": "completed"},
      {"original_name": "photo2.jpg", "status": "completed"},
      {"original_name": "photo3.jpg", "status": "completed"}
    ]
  },
  "current_operation": "Processing video.mp4"
}
```

**Final Status (Completed)**:
```json
{
  "job_id": "job_abc123def456",
  "status": "completed",
  "duration_seconds": 8,
  "total_items": 5,
  "processed_items": 5,
  "progress": 100,
  "results": {
    "media": [
      {"original_name": "photo1.jpg", "status": "completed", "path": "..."},
      {"original_name": "photo2.jpg", "status": "completed", "path": "..."},
      {"original_name": "photo3.jpg", "status": "completed", "path": "..."},
      {"original_name": "video.mp4", "status": "completed", "path": "..."},
      {"original_name": "document.pdf", "status": "completed", "path": "..."}
    ]
  }
}
```

**Highlight**:
- âœ… Background processing with 10 workers
- âœ… Real-time progress tracking
- âœ… 1000+ jobs/sec throughput
- âœ… Client never blocks (vs 8 seconds synchronous wait)

---

### ðŸŽ¬ Scenario 7: Query & Retrieve Files

**Objective**: Show file search and retrieval

```bash
# Search for files by name
curl "http://localhost:8090/files?name=photo"
```

**Expected Response**:
```json
{
  "files": [
    {
      "path": "storage/media/images/jpg/vacation_photos/abc123...photo.jpg",
      "name": "photo.jpg",
      "size": 2457600,
      "mime_type": "image/jpeg",
      "uploaded_at": "2025-11-15T10:30:00Z"
    },
    {
      "path": "storage/media/images/jpg/photo1.jpg",
      "name": "photo1.jpg",
      "size": 1234567,
      "mime_type": "image/jpeg",
      "uploaded_at": "2025-11-15T10:32:00Z"
    }
  ],
  "total": 2,
  "page": 1,
  "per_page": 20
}
```

**Download File**:
```bash
# Download specific file
curl -O "http://localhost:8090/files/storage/media/images/jpg/vacation_photos/abc123...photo.jpg"

# Verify downloaded
file photo.jpg
# Expected: photo.jpg: JPEG image data, ...
```

**Highlight**:
- âœ… Full-text search across filenames
- âœ… Pagination support
- âœ… Direct file download
- âœ… Streaming for large files

---

## Performance Demo

### High-Volume Batch Test

```bash
# Generate 1000 dummy JSON documents
cat > batch1000.json <<EOF
{
  "namespace": "performance_test",
  "documents": [
$(for i in {1..1000}; do
    echo "    {\"id\": $i, \"name\": \"Item $i\", \"value\": $(($RANDOM % 1000))}"
    [ $i -lt 1000 ] && echo "    ,"
done)
  ]
}
EOF

# Upload 1000 documents and measure time
time curl -X POST http://localhost:8090/ingest \
  -H "Content-Type: application/json" \
  -d @batch1000.json
```

**Expected Result**:
- **Time**: 50-100ms for 1000 inserts
- **Throughput**: 10K-20K inserts/sec
- **Database**: PostgreSQL COPY protocol or MongoDB BulkWrite

**Highlight**:
- âœ… 1000 documents in <100ms
- âœ… 100K+/sec PostgreSQL COPY throughput
- âœ… 200K+/sec MongoDB BulkWrite throughput

---

## Summary Table

| Demo | Feature Showcased | Key Metric |
|------|-------------------|------------|
| 1. Image Upload | MIME detection, type-based storage | <50ms latency |
| 2. Video Upload | Multi-type support, large files | 100MB+ files |
| 3. JSON â†’ SQL | Intelligent SQL routing | 95% confidence, auto-schema |
| 4. JSON â†’ NoSQL | Intelligent NoSQL routing | Flexible schema support |
| 5. Deduplication | Content-addressed storage | 50% storage savings, <1ms detection |
| 6. Async Batch | Background processing, progress tracking | 0ms client blocking, 1677 jobs/sec |
| 7. Query/Retrieve | File search and download | Full-text search, streaming |
| 8. Performance | High-volume batch insert | 10K-20K inserts/sec |

---

## Presentation Tips

### Opening (30 seconds)

> "RhinoBox solves the universal storage challenge: **one API, any data type**. Upload images, videos, JSONâ€”RhinoBox intelligently routes to the right storage: file system for media, PostgreSQL for relational data, MongoDB for flexible documents. Let me show you."

### During Demo

1. **Show the curl command first** (so audience can follow)
2. **Highlight the response** (point out key fields)
3. **Verify the result** (show database/filesystem)
4. **State the metric** (latency, throughput, savings)

### Key Talking Points

- **Problem**: Most systems have separate APIs for different data types
- **Solution**: Single unified `/ingest` endpoint with intelligent routing
- **Innovation**: Automatic SQL vs NoSQL decision based on schema analysis
- **Performance**: 1000+ files/sec, 100K-200K DB inserts/sec
- **Deduplication**: SHA-256 content addressing saves 50%+ storage

### Closing (30 seconds)

> "In summary: RhinoBox provides **one API for everything**â€”images, videos, JSONâ€”with **intelligent routing** to optimal storage. It's **production-ready** with 1000+ files/sec throughput, automatic deduplication, and battle-tested technologies. All **open-source** and **ready to deploy with Docker Compose**."

---

## Backup Scenarios (If Time Permits)

### Document Upload

```bash
curl -X POST http://localhost:8090/ingest \
  -F "files=@document.pdf" \
  -F "comment=Project proposal"
```

### Mixed Batch

```bash
curl -X POST http://localhost:8090/ingest \
  -F "files=@photo.jpg" \
  -F "files=@video.mp4" \
  -F "files=@document.pdf"
```

### File Metadata Update

```bash
curl -X PATCH http://localhost:8090/files/storage/media/images/jpg/photo.jpg \
  -H "Content-Type: application/json" \
  -d '{"comment": "Updated comment: Best vacation ever!"}'
```

---

## Q&A Preparation

**Q: How does it decide SQL vs NoSQL?**  
A: Schema analyzer looks at: field consistency (>80% = SQL), foreign keys (`*_id` pattern), nesting depth (>3 = NoSQL), and comment hints.

**Q: What if database goes down?**  
A: Graceful degradationâ€”continues with NDJSON-only mode. All data still saved to filesystem as backup.

**Q: Can it scale horizontally?**  
A: Yes, it's stateless. Deploy multiple instances behind a load balancer with shared PostgreSQL/MongoDB/S3 storage.

**Q: What about security?**  
A: Production deployment should add: API authentication, TLS/SSL, rate limiting, and firewall rules. Hackathon version focuses on core functionality.

**Q: Performance vs competitors?**  
A: 100x faster than naive INSERT statements via COPY protocol. 4-6x more cost-efficient than Python/Node.js alternatives.

---

## Demo Environment Reset

```bash
# Clean slate for next demo
docker-compose down -v
docker-compose up -d
sleep 30

# Verify
curl http://localhost:8090/healthz
```

---

**Good luck with your presentation! ðŸš€**
