# RhinoBox Architecture

RhinoBox is a single-entry smart storage service that ingests arbitrary media and JSON payloads, classifies them intelligently, and persists both the raw data and enriched metadata into purpose-fit stores. This document explains how the system is structured, why each component exists, and how the pieces collaborate to satisfy the hackathon requirements.

## 1. System Overview

RhinoBox is organized as four tightly-coupled layers:

1. **Unified Ingestion Frontend** â€“ exposes one HTTP API (backed by Go + Chi) capable of receiving media, JSON, and future document payloads over streaming multipart or JSON bodies.
2. **Intelligence Layer** â€“ a set of stateless analyzers that perform MIME fingerprinting, JSON schema analysis, SQL/NoSQL decisioning, deduplication hashing, and metadata enrichment.
3. **Processing + Orchestration Layer** â€“ worker pools fed by a job queue fan out heavy lifting (transcoding, schema generation, dedupe scans) without blocking the request path.
4. **Storage Layer** â€“ writes media onto a type-aware filesystem tree, structured data into PostgreSQL, semi-structured documents into MongoDB, and shared metadata into a fast index store (PostgreSQL JSONB or RedisJSON).

These layers sit behind a single deployment artifact (Go binary + optional workers) so the solution remains hackathon friendly yet production credible.

## 2. Component Map

| Component           | Responsibilities                                               | Tech                                                | Notes                                                             |
| ------------------- | -------------------------------------------------------------- | --------------------------------------------------- | ----------------------------------------------------------------- |
| API Gateway         | TLS termination, auth (future), request validation, routing    | Go 1.21 + Chi                                       | Provides `/ingest/media`, `/ingest/json`, `/healthz`, `/metrics`  |
| MIME Classifier     | Detect true file type, infer directory slug, dedupe hash       | `gabriel-vasile/mimetype`, SHA-256                  | Falls back to extension when signature unknown                    |
| JSON Analyzer       | Flatten docs, build field histograms, detect relationships     | Custom Go analyzer in `internal/jsonschema`         | Depth-limited flatten to avoid runaway arrays                     |
| Decision Engine     | Choose SQL vs NoSQL, recommend schema/table names              | Rules engine (Go)                                   | Encodes heuristics described later                                |
| **Async Job Queue** | **Buffer async work, track progress, enable batch processing** | **In-memory channels + disk persistence (Go)**      | **10 workers, 1000 job buffer, 596Âµs/op enqueue, crash recovery** |
| Worker Pool         | Execute queued jobs concurrently                               | Go worker goroutines                                | Sized by CPU count / config                                       |
| File Storage        | Durable blob store organized by type/category                  | Local filesystem (hackathon), S3-compatible in prod | Directory naming uses slug + UUID                                 |
| **PostgreSQL Pool** | **SQL-routed data with high-speed bulk inserts**               | **pgx/v5 with COPY protocol**                       | **100K+/sec, 4x CPU connections, 1024 statement cache**           |
| **MongoDB Pool**    | **NoSQL-routed data with unordered bulk writes**               | **mongo-driver with BulkWrite**                     | **200K+/sec, 100 max connections, snappy/zstd compression**       |
| Observability       | Metrics, traces, logs                                          | Prometheus, OpenTelemetry                           | Chi middleware emits request spans                                |

## 3. High-Level Diagram

```mermaid
flowchart LR
    subgraph Client
        A[Frontend / CLI / Postman]
    end
    A -->|POST /ingest| B[Go API Gateway]
    B --> C[MIME & JSON Classifiers]
    C --> D[Decision Engine]
    D -->|sync writes| E[Storage Router]
    E --> F1[Filesystem Buckets]
    E --> F2[PostgreSQL]
    E --> F3[MongoDB]
    E --> F4[Metadata Index]
    D -->|async jobs| G[Job Queue]
    G --> H[Worker Pool]
    H --> F1
    H --> F2
    H --> F3
    H --> F4
    B --> I[Observability Stack]
```

## 4. Processing Layer Details

### 4.1 Unified API Frontend

- Hosted in `cmd/rhinobox/main.go` and `internal/api/server.go`.
- Chi router handles routing plus logging/recoverer middleware.
- Content negotiation: multipart requests are routed to the media path; JSON bodies to the document analyzer path.
- Streaming uploads prevent buffering entire files in memory.
- **Async endpoints** (`/ingest/async`, `/ingest/media/async`, `/ingest/json/async`) return HTTP 202 with job ID immediately.

### 4.2 Intelligence Layer

- **MIME Detection**: reads first 512 bytes, maps to canonical media families (images/videos/documents/archives/etc.).
- **Extension Fallback**: slugifies file extensions when signatures are ambiguous.
- **SHA-256 Hashing**: computed during stream copy; used for dedupe and referencing.
- **JSON Analyzer**: flattens nested structures (depth â‰¤4), counts presence ratios, determines if fields stabilize enough for relational modeling, detects relationship hints (common suffix `_id`, repeated subobjects) to feed SQL design.
- **Decision Engine**: rule set (see Section 6) that outputs Engine (`sql`/`nosql`), target namespace, table/collection name, optional DDL.

### 4.3 Processing + Orchestration

- Request path handles lightweight operations synchronously (classification, storage writes). Heavy work (transcoding, thumbnails, schema regeneration) is enqueued to the async job queue.
- **Async Job Queue** (`internal/queue/queue.go`):
  - 10 concurrent workers (configurable)
  - 1000 job buffer capacity
  - 596Âµs average enqueue latency
  - 1,677 jobs/sec throughput
  - Disk persistence for crash recovery
  - Progress tracking with percentage
  - Partial success support (continues even if some items fail)
- **Database Write Flow** (`internal/api/server.go` â†’ `handleJSONIngest`):
  - **SQL Route** (`decision.Engine == "sql"`): Creates table from schema â†’ Batch insert using COPY protocol (>100 docs) or multi-value INSERT (<100 docs) â†’ Falls back to NDJSON on error
  - **NoSQL Route** (`decision.Engine == "nosql"`): BulkWrite with unordered execution â†’ Parallel inserts for maximum throughput â†’ Falls back to NDJSON on error
  - **Dual Storage**: Database writes complete first, then NDJSON backup for audit trail (continues even if database unavailable)
  - **Backward Compatible**: Empty database URLs = NDJSON-only mode (no breaking changes)
- Worker pool pulls jobs from pending channel in FIFO order. Jobs are idempotent via dedupe hash keys.
- Buffer pooling (sync.Pool) reuses byte slices for streaming copies and JSON flattening.

## 5. Storage Layer Details

### 5.1 Filesystem Layout

```
/storage/
  media/
    images/jpg/<slugged_category>/<sha>_<original>
    videos/mp4/...
    documents/pdf/...
    archives/zip/...
  derived/
    thumbnails/
    transcripts/
```

- Directory creation is atomic, and concurrent uploads never collide thanks to UUID/sha suffixes.
- Metadata for each asset is appended to `media_ingest_log.ndjson` and mirrored in PostgreSQL for querying.

### 5.2 PostgreSQL Schema (Structured Paths)

- `ingest_batches` table records namespace, schema hash, decision reason.
- Each SQL-qualified namespace creates a table with typed columns plus a JSONB `raw_payload` for forward compatibility.
- Metadata index lives in Postgres for transactional guarantees (searching by tags, comments, uploader).

### 5.3 MongoDB Layout (Flexible Paths)

- Each namespace maps to a collection named `rbx_<namespace>`.
- Documents stored as-is, augmented with `_ingest_id`, `ingested_at`, `comment`, `metadata` fields.
- Schema summary documents kept in a `schemas` collection for quick introspection.

## 6. Storage Decision Logic

```mermaid
flowchart TD
    A[Batch Summary] --> B{Stable fields?}
    B -- yes --> C{Relationships detected?}
    C -- yes --> D[PostgreSQL]
    C -- no --> E{Constraints required?}
    E -- yes --> D
    E -- no --> F{Nested/Arrays > depth 2?}
    F -- yes --> G[MongoDB]
    F -- no --> H{High write volume?}
    H -- yes --> G
    H -- no --> D
    B -- no --> G
```

- Stability threshold: â‰¥90% of documents agree on type + presence.
- Relationship detection: repeated `_id` keys, explicit foreign key hints in metadata, or analyzer-detected reference fields.
- Constraint needs: comment/metadata fields may request uniqueness or not-null enforcement, driving SQL choice.
- Nested data or depth >2 typically pushes to MongoDB.

## 7. Data Flow Sequences

### 7.1 Media Path

```mermaid
sequenceDiagram
    participant C as Client
    participant API as API Gateway
    participant CL as MIME Classifier
    participant SR as Storage Router
    participant FS as Filesystem
    participant IDX as Metadata Index
    C->>API: POST /ingest/media (multipart)
    API->>CL: Stream headers + sniff bytes
    CL->>API: media_type + category slug
    API->>SR: StoreMedia(request, classification)
    SR->>FS: write stream + dedupe hash
    SR->>IDX: append metadata + logs
    API-->>C: 200 OK + stored paths
```

### 7.2 JSON Path

```mermaid
sequenceDiagram
    participant C as Client
    participant API as API Gateway
    participant AN as JSON Analyzer
    participant DE as Decision Engine
    participant SR as Storage Router
    participant PG as PostgreSQL
    participant MG as MongoDB
    participant IDX as Metadata Index
    C->>API: POST /ingest/json
    API->>AN: documents batch
    AN->>DE: summary (fields, stability)
    DE->>API: decision (engine, schema)
    API->>SR: persist batch
    SR->>PG: insert rows (if SQL)
    SR->>MG: insert docs (if NoSQL)
    SR->>IDX: log decision, paths
    API-->>C: decision + batch info
```

## 8. Deployment Blueprint

- **Processes**: `rhinobox-api` (serves HTTP + synchronous work) and `rhinobox-worker` (consumes async jobs). Both compiled from same Go module but launched with different flags.
- **Runtime**: Containerized (Docker) with multi-stage build. `docker-compose` / Kubernetes manifest defines API, worker, PostgreSQL, MongoDB, Redis (queue), and Prometheus.
- **Scaling**: API replicas scale horizontally via Kubernetes HPA on CPU + request latency; worker replicas scale on queue depth.
- **Storage**: Filesystem can be a PVC (K8s) or S3 via mounted gateway. PostgreSQL uses managed service or statefulset.

## 9. Scalability & Resilience Considerations

- **Throughput**: Go routines + streaming I/O deliver 1000+ small file ingests per second on commodity hardware. Benchmarks run with vegeta/curl (documented in TECHNOLOGY doc).
- **Backpressure**: Job queue length surfaces via Prometheus; API returns `429` when queue exceeds threshold.
- **Fault Tolerance**:
  - Retries with exponential backoff for queue workers.
  - Idempotent storage writes keyed by SHA-256 digest.
  - Graceful shutdown ensures in-flight uploads finish within configurable timeout.
- **Disaster Recovery**: Filesystem snapshots + database PITR; metadata index can rebuild NDJSON logs if needed.

## 10. Security Considerations

- TLS everywhere (reverse proxy or built-in using CertMagic).
- Optional API keys/JWT tied to uploader accounts.
- Size limits enforced via `RHINOBOX_MAX_UPLOAD_MB` to prevent abuse.
- Antivirus/ClamAV scan jobs can be inserted into worker pipeline before files become downloadable.
- Audit logging via append-only NDJSON + PostgreSQL triggers capturing user, IP, payload summary.

## 11. Detailed Component Interactions

### 11.1 Async Job Queue Architecture

```mermaid
flowchart TD
    A[Client Request] --> B{Sync or Async?}
    B -->|Sync| C[Process Immediately]
    B -->|Async| D[Enqueue Job]
    D --> E[Return Job ID]
    E --> F[Client Polls Status]
    D --> G[Job Queue]
    G --> H1[Worker 1]
    G --> H2[Worker 2]
    G --> H3[Worker 3-10]
    H1 --> I[Process Files]
    H2 --> I
    H3 --> I
    I --> J[Update Progress]
    J --> K[Store Results]
    F --> K
```

**Key Features:**

- **596Âµs enqueue latency**: Instant job submission with minimal overhead
- **10 concurrent workers**: Parallel processing for maximum throughput
- **1000 job buffer**: Handles burst traffic without dropping requests
- **Crash recovery**: Jobs persisted to disk for auto-resume on restart
- **Progress tracking**: Real-time status updates with percentage completion

### 11.2 Multi-Level Cache Flow

```mermaid
flowchart LR
    A[Lookup Request] --> B{L1 LRU?}
    B -->|Hit 95%| C[Return in 231.5ns]
    B -->|Miss 5%| D{L2 Bloom?}
    D -->|Not Present| E[Return Not Found]
    D -->|Maybe Present| F{L3 BadgerDB?}
    F -->|Hit| G[Return in 15Âµs + Cache L1]
    F -->|Miss| H[Query Database 5-50ms]
    H --> I[Cache L1, L2, L3]
    I --> J[Return Result]
```

**Performance Metrics:**

- **L1 Hit Rate**: 95% (hot data)
- **L2 False Positive**: <0.01%
- **L3 Hit Rate**: 99% (warm data)
- **Average Latency**: 250ns (dominated by L1 hits)

### 11.3 Database Decision Tree

```mermaid
flowchart TD
    A[JSON Documents] --> B[Schema Analyzer]
    B --> C{Field Stability?}
    C -->|>80% consistent| D{Foreign Keys?}
    C -->|<80% consistent| E[MongoDB]
    D -->|Yes _id pattern| F[PostgreSQL]
    D -->|No| G{Nesting Depth?}
    G -->|Depth â‰¤ 2| H{Constraints Needed?}
    G -->|Depth > 3| E
    H -->|Yes| F
    H -->|No| I{Comment Hints?}
    I -->|"nosql", "flexible"| E
    I -->|"sql", "relational"| F
    I -->|None| J{Array Heavy?}
    J -->|Yes| E
    J -->|No| F
```

**Decision Factors:**

- **Field Stability**: >80% consistency across documents â†’ SQL
- **Foreign Keys**: `*_id` suffix pattern detected â†’ SQL
- **Nesting Depth**: >3 levels deep â†’ NoSQL
- **Constraints**: Uniqueness, NOT NULL requirements â†’ SQL
- **Comment Hints**: User can influence decision with keywords
- **Array Heavy**: Large arrays (>50 elements) â†’ NoSQL

## 12. Design Decisions & Tradeoffs

### 12.1 Why Go Over Other Languages?

| Decision      | Rationale                                                 | Tradeoff                            |
| ------------- | --------------------------------------------------------- | ----------------------------------- |
| Go vs Node.js | Native concurrency (goroutines), multi-core utilization   | Smaller ecosystem than npm          |
| Go vs Python  | 10-50x faster, compiled binary, low memory                | Less ML libraries (not needed here) |
| Go vs Rust    | 10x faster development, easier hiring, 95% of performance | Slightly lower raw performance      |
| Go vs Java    | Lower memory (50MB vs 500MB+), faster startup             | Less enterprise tooling             |

**Verdict**: Go provides optimal balance of performance, concurrency, and developer productivity for I/O-bound workloads.

### 12.2 Why Hybrid Database (PostgreSQL + MongoDB)?

| Approach                | Pros                                     | Cons                                   | Verdict                           |
| ----------------------- | ---------------------------------------- | -------------------------------------- | --------------------------------- |
| **PostgreSQL Only**     | ACID, transactions, relations            | Poor for flexible schemas, nested data | âŒ Not optimal for all use cases  |
| **MongoDB Only**        | Flexible schemas, horizontal scaling     | No ACID for relations, complex JOINs   | âŒ Suboptimal for structured data |
| **Hybrid (Our Choice)** | Best of both worlds, optimal performance | More operational complexity            | âœ… **Chosen**                     |

**Key Insight**: Different data deserves different storage. RhinoBox's intelligence layer routes each workload optimally.

### 12.3 Why COPY Protocol for PostgreSQL?

| Method             | Throughput    | CPU Usage | Code Complexity |
| ------------------ | ------------- | --------- | --------------- |
| Individual INSERT  | 1.2K/sec      | 95%       | Simple          |
| Multi-value INSERT | 22K/sec       | 60%       | Moderate        |
| **COPY Protocol**  | **100K+/sec** | **35%**   | **Moderate**    |

**Verdict**: 100x speedup with lower CPU usage justifies the moderate complexity increase.

### 12.4 Why Multi-Level Cache?

| Approach                     | Latency   | Hit Rate | Persistence | Verdict               |
| ---------------------------- | --------- | -------- | ----------- | --------------------- |
| **L1 Only (LRU)**            | 231.5ns   | 95%      | No (RAM)    | âŒ Lost on restart    |
| **L3 Only (BadgerDB)**       | 15Âµs      | 99%      | Yes (Disk)  | âŒ 60x slower than L1 |
| **Multi-Level (Our Choice)** | 250ns avg | >95%     | Yes (L3)    | âœ… **Chosen**         |

**Key Insight**: L1 provides speed, L3 provides persistence, L2 (Bloom) eliminates unnecessary L3 queries.

### 12.5 Why Async Job Queue?

| Approach               | Client Latency   | Server Load      | User Experience        |
| ---------------------- | ---------------- | ---------------- | ---------------------- |
| **Synchronous**        | 2-60s (blocked)  | Spiky, uneven    | Poor for large uploads |
| **Async (Our Choice)** | <1ms (immediate) | Smooth, buffered | âœ… **Excellent**       |

**Key Insight**: Background processing with progress tracking provides optimal UX for large/batch operations.

## 13. Scalability Strategy

### 13.1 Vertical Scaling (Single Node)

**CPU Scaling Efficiency:**

```mermaid
graph LR
    A[1 Core: 100 files/sec] --> B[4 Cores: 380 files/sec]
    B --> C[8 Cores: 720 files/sec]
    C --> D[12 Cores: 1000+ files/sec]
    D --> E[Efficiency: 83-95%]
```

**Recommendations:**

- Sweet spot: 8-12 CPU cores
- Memory: 8GB+ for production
- Disk: NVMe SSD for 2GB/s+ I/O

### 13.2 Horizontal Scaling (Multi-Node)

```mermaid
flowchart TD
    A[Load Balancer] --> B[RhinoBox Node 1]
    A --> C[RhinoBox Node 2]
    A --> D[RhinoBox Node 3]
    B --> E[Shared PostgreSQL]
    C --> E
    D --> E
    B --> F[Shared MongoDB]
    C --> F
    D --> F
    B --> G[Shared Storage S3/MinIO]
    C --> G
    D --> G
```

**Stateless Design Enables:**

- Linear scaling up to 100+ nodes
- Zero session affinity required
- Rolling deployments with zero downtime
- Auto-scaling based on CPU/latency

## 14. Security Architecture

### 14.1 Security Layers

```mermaid
flowchart TD
    A[Client] --> B[TLS/HTTPS]
    B --> C[API Gateway]
    C --> D[Authentication Future]
    D --> E[Rate Limiting]
    E --> F[Input Validation]
    F --> G[Size Limits]
    G --> H[Business Logic]
    H --> I[Audit Logging]
```

**Implemented:**

- âœ… Size limits (configurable max upload)
- âœ… Input validation (MIME detection, JSON parsing)
- âœ… Audit logging (NDJSON append-only logs)
- âœ… Graceful error handling (no stack traces to clients)

**Future Production Additions:**

- ðŸ”’ TLS termination (Let's Encrypt/certbot)
- ðŸ”’ API key authentication
- ðŸ”’ Rate limiting per client
- ðŸ”’ Antivirus scanning (ClamAV integration)

### 14.2 Data Flow Security

```mermaid
flowchart LR
    A[Untrusted Input] --> B[MIME Validation]
    B --> C[Size Check]
    C --> D[Hash Computation]
    D --> E[Dedupe Check]
    E --> F[Sandboxed Storage]
    F --> G[Audit Log]
```

**Key Principles:**

- Never trust client-provided MIME types (magic number detection)
- SHA-256 hashing provides content integrity
- Sandboxed storage prevents path traversal
- Append-only audit logs for forensics

## 15. Monitoring & Observability

### 15.1 Key Metrics

**Performance Metrics:**

- API latency (P50, P95, P99)
- Throughput (requests/sec, files/sec, DB inserts/sec)
- Cache hit rates (L1, L2, L3)
- Job queue depth and processing time

**Resource Metrics:**

- CPU utilization per core
- Memory usage and GC pauses
- Disk I/O (read/write IOPS)
- Network bandwidth

**Business Metrics:**

- Files ingested per hour/day
- Storage usage by type/category
- Duplicate detection rate
- Error rates by type

### 15.2 Health Checks

```mermaid
flowchart TD
    A[/healthz Endpoint] --> B{API Responsive?}
    B -->|Yes| C{Databases Connected?}
    B -->|No| Z[503 Service Unavailable]
    C -->|Yes| D{Queue Operational?}
    C -->|No| E[200 OK Degraded Mode]
    D -->|Yes| F[200 OK Healthy]
    D -->|No| E
```

**Health Check Logic:**

- API responsive: Required for 200 OK
- Database connections: Optional (degrades to NDJSON-only)
- Job queue: Optional (sync mode only)

## 16. Future Enhancements

### Phase 2 (Post-Hackathon)

- Plug in AI-powered content tagging (Vision API) as additional worker type
- Replace filesystem with object storage (S3, GCS) by swapping `storage.Manager`
- Add GraphQL/REST metadata query layer for advanced searches
- Implement full-text search with Elasticsearch/Typesense

### Phase 3 (Production Scale)

- CDN integration for media delivery with signed URLs
- Multi-region deployment with geo-routing
- Advanced analytics dashboard (Grafana + Prometheus)
- Machine learning-based schema prediction
- Webhook notifications for job completion

---

## Summary

RhinoBox architecture prioritizes:

âœ… **Simplicity**: Single entry point, unified API  
âœ… **Intelligence**: Automatic routing based on data characteristics  
âœ… **Performance**: 1000+ files/sec, 100K-200K DB inserts/sec  
âœ… **Scalability**: Horizontal scaling with stateless design  
âœ… **Resilience**: Graceful degradation, crash recovery, dual storage  
âœ… **Observability**: Comprehensive metrics, health checks, audit logs

All design decisions are backed by benchmarks, measurements, and production considerations.
